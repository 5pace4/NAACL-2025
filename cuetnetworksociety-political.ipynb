{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n                          Trainer, TrainingArguments)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom datasets import Dataset\nfrom indicnlp.normalize.indic_normalize import IndicNormalizerFactory\nimport time\n\n# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load Data\ntrain_data = pd.read_csv('/kaggle/input/ps-dataset/PS_train.csv') \nval_data = pd.read_csv('/kaggle/input/ps-dataset/PS_dev.csv')\ntest_data = pd.read_csv('/kaggle/input/ps-dataset/PS_test_without_lables.csv')\n\n# Handle missing values\ntrain_data.dropna(inplace=True)\nval_data.dropna(inplace=True)\ntest_data.dropna(inplace=True)\n\n# Sample 10% of data to reduce training time\ntrain_data = train_data.sample(frac=0.1, random_state=42)\n\n# Normalization\nnormalizer_factory = IndicNormalizerFactory()\nnormalizer = normalizer_factory.get_normalizer(\"ta\")\n\ndef normalize_text(text):\n    return normalizer.normalize(text)\n\ntrain_data['content'] = train_data['content'].apply(normalize_text)\nval_data['content'] = val_data['content'].apply(normalize_text)\ntest_data['content'] = test_data['content'].apply(normalize_text)\n\n# Truncate long texts to 512 chars\ntrain_data['content'] = train_data['content'].apply(lambda x: x[:512])\nval_data['content'] = val_data['content'].apply(lambda x: x[:512])\ntest_data['content'] = test_data['content'].apply(lambda x: x[:512])\n\n# Encode labels to integers\nlabel_encoder = LabelEncoder()\ntrain_data['labels'] = label_encoder.fit_transform(train_data['labels'])\nval_data['labels'] = label_encoder.transform(val_data['labels'])\n\n# Train-Test Split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_data['content'], train_data['labels'], test_size=0.1, random_state=42\n)\n\n# Dataset Class\nclass TamilDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n# Train and Evaluate Model\ndef train_and_evaluate(model_name):\n    print(f\"Training {model_name}...\")\n    start = time.time()\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Tokenize Data\n    train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=256)\n    val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=256)\n    \n    # Create Datasets\n    train_dataset = TamilDataset(train_encodings, train_labels.tolist())\n    val_dataset = TamilDataset(val_encodings, val_labels.tolist())\n    \n    # Load Model\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(train_labels)))\n    model.to(device)\n    \n    # Training Arguments\n    training_args = TrainingArguments(\n        output_dir=f\"./results/{model_name}\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        num_train_epochs=1,\n        logging_dir=f\"./logs/{model_name}\",\n        logging_steps=200,\n        fp16=torch.cuda.is_available(),\n        report_to=\"none\"\n    )\n    \n    # Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer\n    )\n    \n    # Train and Evaluate\n    trainer.train()\n    predictions = trainer.predict(val_dataset).predictions.argmax(axis=1)\n    accuracy = accuracy_score(val_labels, predictions)\n    print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n    print(classification_report(val_labels, predictions))\n    print(\"Training Time:\", time.time() - start)\n\n    return predictions\n\n# Use a Lightweight Model\nmodel_name = \"distilbert-base-multilingual-cased\"\n\ntrain_and_evaluate(model_name)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}